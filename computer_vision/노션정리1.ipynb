{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20000 W: 0.650000, b: 0.147 Cost: 70.333336\n",
      "Epoch  100/20000 W: 2.126379, b: 0.060 Cost: 0.712527\n",
      "Epoch  200/20000 W: 2.192670, b: -0.224 Cost: 0.626169\n",
      "Epoch  300/20000 W: 2.238662, b: -0.421 Cost: 0.584604\n",
      "Epoch  400/20000 W: 2.270569, b: -0.557 Cost: 0.564597\n",
      "Epoch  500/20000 W: 2.292706, b: -0.652 Cost: 0.554968\n",
      "Epoch  600/20000 W: 2.308064, b: -0.718 Cost: 0.550333\n",
      "Epoch  700/20000 W: 2.318718, b: -0.763 Cost: 0.548102\n",
      "Epoch  800/20000 W: 2.326110, b: -0.795 Cost: 0.547028\n",
      "Epoch  900/20000 W: 2.331239, b: -0.817 Cost: 0.546511\n",
      "Epoch 1000/20000 W: 2.334797, b: -0.832 Cost: 0.546263\n",
      "Epoch 1100/20000 W: 2.337265, b: -0.843 Cost: 0.546143\n",
      "Epoch 1200/20000 W: 2.338977, b: -0.850 Cost: 0.546085\n",
      "Epoch 1300/20000 W: 2.340165, b: -0.855 Cost: 0.546058\n",
      "Epoch 1400/20000 W: 2.340990, b: -0.859 Cost: 0.546044\n",
      "Epoch 1500/20000 W: 2.341561, b: -0.861 Cost: 0.546038\n",
      "Epoch 1600/20000 W: 2.341958, b: -0.863 Cost: 0.546035\n",
      "Epoch 1700/20000 W: 2.342233, b: -0.864 Cost: 0.546033\n",
      "Epoch 1800/20000 W: 2.342424, b: -0.865 Cost: 0.546032\n",
      "Epoch 1900/20000 W: 2.342557, b: -0.865 Cost: 0.546032\n",
      "Epoch 2000/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 2100/20000 W: 2.342712, b: -0.866 Cost: 0.546032\n",
      "Epoch 2200/20000 W: 2.342757, b: -0.866 Cost: 0.546032\n",
      "Epoch 2300/20000 W: 2.342787, b: -0.866 Cost: 0.546032\n",
      "Epoch 2400/20000 W: 2.342809, b: -0.866 Cost: 0.546032\n",
      "Epoch 2500/20000 W: 2.342824, b: -0.867 Cost: 0.546032\n",
      "Epoch 2600/20000 W: 2.342834, b: -0.867 Cost: 0.546032\n",
      "Epoch 2700/20000 W: 2.342841, b: -0.867 Cost: 0.546032\n",
      "Epoch 2800/20000 W: 2.342845, b: -0.867 Cost: 0.546032\n",
      "Epoch 2900/20000 W: 2.342849, b: -0.867 Cost: 0.546032\n",
      "Epoch 3000/20000 W: 2.342851, b: -0.867 Cost: 0.546032\n",
      "Epoch 3100/20000 W: 2.342852, b: -0.867 Cost: 0.546032\n",
      "Epoch 3200/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 3300/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 3400/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 3500/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 3600/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 3700/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 3800/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 3900/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 4000/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 4100/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 4200/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 4300/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 4400/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 4500/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 4600/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 4700/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 4800/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 4900/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 5000/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 5100/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 5200/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 5300/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 5400/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 5500/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 5600/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 5700/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 5800/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 5900/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 6000/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 6100/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 6200/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 6300/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 6400/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 6500/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 6600/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 6700/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 6800/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 6900/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 7000/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 7100/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 7200/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 7300/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 7400/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 7500/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 7600/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 7700/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 7800/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 7900/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 8000/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 8100/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 8200/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 8300/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 8400/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 8500/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 8600/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 8700/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 8800/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 8900/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 9000/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 9100/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 9200/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 9300/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 9400/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 9500/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 9600/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 9700/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 9800/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 9900/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 10000/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 10100/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 10200/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 10300/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 10400/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 10500/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 10600/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 10700/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 10800/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 10900/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 11000/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 11100/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 11200/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 11300/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 11400/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 11500/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 11600/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 11700/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 11800/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 11900/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 12000/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 12100/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 12200/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 12300/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 12400/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 12500/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 12600/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 12700/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 12800/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 12900/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 13000/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 13100/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 13200/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 13300/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 13400/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 13500/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 13600/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 13700/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 13800/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 13900/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 14000/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 14100/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 14200/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 14300/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 14400/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 14500/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 14600/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 14700/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 14800/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 14900/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 15000/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 15100/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 15200/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 15300/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 15400/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 15500/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 15600/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 15700/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 15800/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 15900/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 16000/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 16100/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 16200/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 16300/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 16400/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 16500/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 16600/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 16700/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 16800/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 16900/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 17000/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 17100/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 17200/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 17300/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 17400/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 17500/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 17600/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 17700/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 17800/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 17900/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 18000/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 18100/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 18200/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 18300/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 18400/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 18500/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 18600/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 18700/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 18800/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 18900/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 19000/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 19100/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 19200/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 19300/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 19400/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 19500/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 19600/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 19700/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 19800/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 19900/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n",
      "Epoch 20000/20000 W: 2.342853, b: -0.867 Cost: 0.546032\n"
     ]
    }
   ],
   "source": [
    "#변수선언\n",
    "x_train = torch.FloatTensor([[1],[2],[3],[4],[5],[6]])\n",
    "y_train = torch.FloatTensor([[2],[4],[6],[7],[11],[14]])\n",
    "\n",
    "#가중치, 편향초기화\n",
    "#requires_grad=True는 해당 변수에 gradient값이 저장되고 업데이트 된다는 의미\n",
    "w = torch.zeros(1,requires_grad=True)\n",
    "b = torch.zeros(1,requires_grad=True) \n",
    "\n",
    "#경사하강법 구현\n",
    "optimizer = optim.SGD([w,b], lr=0.01)\n",
    "epochs = 20000\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "\n",
    "    H = x_train * w + b\n",
    "    cost = torch.mean((H - y_train)**2)\n",
    "\n",
    "    optimizer.zero_grad() #경사 0으로 초기화\n",
    "    cost.backward() #비용함수 미분해서 gradient 계산\n",
    "    optimizer.step() #w,b 업데이트\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, epochs, w.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 W: 0.650000, b: 0.147 Cost: 70.333336\n",
      "Epoch  100/2000 W: 2.126379, b: 0.060 Cost: 0.712527\n",
      "Epoch  200/2000 W: 2.192670, b: -0.224 Cost: 0.626169\n",
      "Epoch  300/2000 W: 2.238662, b: -0.421 Cost: 0.584604\n",
      "Epoch  400/2000 W: 2.270569, b: -0.557 Cost: 0.564597\n",
      "Epoch  500/2000 W: 2.292706, b: -0.652 Cost: 0.554968\n",
      "Epoch  600/2000 W: 2.308064, b: -0.718 Cost: 0.550333\n",
      "Epoch  700/2000 W: 2.318718, b: -0.763 Cost: 0.548102\n",
      "Epoch  800/2000 W: 2.326110, b: -0.795 Cost: 0.547028\n",
      "Epoch  900/2000 W: 2.331239, b: -0.817 Cost: 0.546511\n",
      "Epoch 1000/2000 W: 2.334797, b: -0.832 Cost: 0.546263\n",
      "Epoch 1100/2000 W: 2.337265, b: -0.843 Cost: 0.546143\n",
      "Epoch 1200/2000 W: 2.338977, b: -0.850 Cost: 0.546085\n",
      "Epoch 1300/2000 W: 2.340165, b: -0.855 Cost: 0.546058\n",
      "Epoch 1400/2000 W: 2.340990, b: -0.859 Cost: 0.546044\n",
      "Epoch 1500/2000 W: 2.341561, b: -0.861 Cost: 0.546038\n",
      "Epoch 1600/2000 W: 2.341958, b: -0.863 Cost: 0.546035\n",
      "Epoch 1700/2000 W: 2.342233, b: -0.864 Cost: 0.546033\n",
      "Epoch 1800/2000 W: 2.342424, b: -0.865 Cost: 0.546032\n",
      "Epoch 1900/2000 W: 2.342557, b: -0.865 Cost: 0.546032\n",
      "Epoch 2000/2000 W: 2.342649, b: -0.866 Cost: 0.546032\n"
     ]
    }
   ],
   "source": [
    "#변수선언\n",
    "x_train = torch.FloatTensor([[1],[2],[3],[4],[5],[6]])\n",
    "y_train = torch.FloatTensor([[2],[4],[6],[7],[11],[14]])\n",
    "\n",
    "#가중치, 편향초기화\n",
    "#requires_grad=True는 해당 변수에 gradient값이 저장되고 업데이트 된다는 의미\n",
    "w = torch.zeros(1,requires_grad=True)\n",
    "b = torch.zeros(1,requires_grad=True) \n",
    "\n",
    "#경사하강법 구현 - 내장함수 사용\n",
    "optimizer = optim.SGD([w,b], lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 2000\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "\n",
    "    H = x_train * w + b\n",
    "    cost = criterion(H,y_train)\n",
    "\n",
    "    optimizer.zero_grad() #새로운 기울기를 기존 기울기에 더하는데 새로운 기울기가 누적되서 계산되지 않게\n",
    "    cost.backward() #비용함수 미분해서 gradient 계산\n",
    "    optimizer.step() #w,b 업데이트\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, epochs, w.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.7206]], requires_grad=True), Parameter containing:\n",
      "tensor([0.9904], requires_grad=True)]\n",
      "Epoch    0/20000 W: 2.342649, b: -0.866 Cost: 22.819906\n",
      "Epoch  100/20000 W: 2.342649, b: -0.866 Cost: 0.962424\n",
      "Epoch  200/20000 W: 2.342649, b: -0.866 Cost: 0.746450\n",
      "Epoch  300/20000 W: 2.342649, b: -0.866 Cost: 0.642497\n",
      "Epoch  400/20000 W: 2.342649, b: -0.866 Cost: 0.592462\n",
      "Epoch  500/20000 W: 2.342649, b: -0.866 Cost: 0.568380\n",
      "Epoch  600/20000 W: 2.342649, b: -0.866 Cost: 0.556788\n",
      "Epoch  700/20000 W: 2.342649, b: -0.866 Cost: 0.551209\n",
      "Epoch  800/20000 W: 2.342649, b: -0.866 Cost: 0.548524\n",
      "Epoch  900/20000 W: 2.342649, b: -0.866 Cost: 0.547231\n",
      "Epoch 1000/20000 W: 2.342649, b: -0.866 Cost: 0.546609\n",
      "Epoch 1100/20000 W: 2.342649, b: -0.866 Cost: 0.546310\n",
      "Epoch 1200/20000 W: 2.342649, b: -0.866 Cost: 0.546166\n",
      "Epoch 1300/20000 W: 2.342649, b: -0.866 Cost: 0.546096\n",
      "Epoch 1400/20000 W: 2.342649, b: -0.866 Cost: 0.546063\n",
      "Epoch 1500/20000 W: 2.342649, b: -0.866 Cost: 0.546047\n",
      "Epoch 1600/20000 W: 2.342649, b: -0.866 Cost: 0.546039\n",
      "Epoch 1700/20000 W: 2.342649, b: -0.866 Cost: 0.546035\n",
      "Epoch 1800/20000 W: 2.342649, b: -0.866 Cost: 0.546033\n",
      "Epoch 1900/20000 W: 2.342649, b: -0.866 Cost: 0.546033\n",
      "Epoch 2000/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 2100/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 2200/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 2300/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 2400/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 2500/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 2600/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 2700/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 2800/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 2900/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 3000/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 3100/20000 W: 2.342649, b: -0.866 Cost: 0.546031\n",
      "Epoch 3200/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 3300/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 3400/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 3500/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 3600/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 3700/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 3800/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 3900/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 4000/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 4100/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 4200/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 4300/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 4400/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 4500/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 4600/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 4700/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 4800/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 4900/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 5000/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 5100/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 5200/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 5300/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 5400/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 5500/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 5600/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 5700/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 5800/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 5900/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 6000/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 6100/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 6200/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 6300/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 6400/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 6500/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 6600/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 6700/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 6800/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 6900/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 7000/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 7100/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 7200/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 7300/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 7400/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 7500/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 7600/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 7700/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 7800/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 7900/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 8000/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 8100/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 8200/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 8300/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 8400/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 8500/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 8600/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 8700/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 8800/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 8900/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 9000/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 9100/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 9200/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 9300/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 9400/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 9500/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 9600/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 9700/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 9800/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 9900/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 10000/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 10100/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 10200/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 10300/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 10400/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 10500/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 10600/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 10700/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 10800/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 10900/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 11000/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 11100/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 11200/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 11300/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 11400/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 11500/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 11600/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 11700/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 11800/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 11900/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 12000/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 12100/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 12200/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 12300/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 12400/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 12500/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 12600/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 12700/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 12800/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 12900/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 13000/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 13100/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 13200/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 13300/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 13400/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 13500/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 13600/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 13700/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 13800/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 13900/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 14000/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 14100/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 14200/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 14300/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 14400/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 14500/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 14600/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 14700/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 14800/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 14900/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 15000/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 15100/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 15200/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 15300/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 15400/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 15500/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 15600/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 15700/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 15800/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 15900/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 16000/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 16100/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 16200/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 16300/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 16400/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 16500/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 16600/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 16700/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 16800/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 16900/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 17000/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 17100/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 17200/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 17300/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 17400/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 17500/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 17600/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 17700/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 17800/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 17900/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 18000/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 18100/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 18200/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 18300/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 18400/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 18500/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 18600/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 18700/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 18800/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 18900/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 19000/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 19100/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 19200/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 19300/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 19400/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 19500/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 19600/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 19700/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 19800/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 19900/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n",
      "Epoch 20000/20000 W: 2.342649, b: -0.866 Cost: 0.546032\n"
     ]
    }
   ],
   "source": [
    "#변수선언\n",
    "x_train = torch.FloatTensor([[1],[2],[3],[4],[5],[6]])\n",
    "y_train = torch.FloatTensor([[2],[4],[6],[7],[11],[14]])\n",
    "\n",
    "#가중치, 편향초기화\n",
    "model = torch.nn.Linear(1,1) #1,1fh 놓은 순간 자동으로 w와 b가 생성되게 됨\n",
    "print(list(model.parameters()))\n",
    "\n",
    "#경사하강법 구현 - 내장함수 사용\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 20000\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "\n",
    "    # H = x_train * w + b\n",
    "    prediction = model(x_train)\n",
    "    cost = criterion(prediction,y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward() #비용함수 미분해서 gradient 계산\n",
    "    optimizer.step() #w,b 업데이트\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, epochs, w.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 1번 코드는 가중치(w,b)와 모델설계(H), loss(cost)를 전부 수동으로 작성했다.\n",
    "+ 2번 코드에서는 loss 부분을 내장함수인 MSELoss로 변환하였다.\n",
    "+ 3번 코드에서는 가중치와 모델설계 부분을 내장함수인 nn.Linear(1,1)로 바꾸었다.\n",
    "\n",
    "3번에서 주의해야 할게 있다면 optim이 기존에는 [w,b]를 수정하도록 작성되있는데 이를\n",
    "자동으로 생성되는 w,b를 가리키는 model.parameter()로 바꿔야 된다는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.)\n",
      "tensor([ 8., 12., 16.])\n",
      "tensor([[ 4.,  8.],\n",
      "        [12., 16.]])\n"
     ]
    }
   ],
   "source": [
    "#자동미분 기능 알아보기\n",
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor([2.0,3.0,4.0],requires_grad=True)\n",
    "c = torch.tensor([[1.0,2.0],[3.0,4.0]],requires_grad=True)\n",
    "\n",
    "x = 2*a**2 + 5\n",
    "y = 2*b**2 + 5\n",
    "z = 2*c**2 + 5\n",
    "\n",
    "x.backward()\n",
    "y.backward(torch.ones_like(b))\n",
    "z.backward(torch.ones_like(c))\n",
    "\n",
    "print(a.grad)\n",
    "print(b.grad)\n",
    "print(c.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 80],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/50 W: 2.342649, b: -0.866 Cost: 48910.714844\n",
      "Epoch    1/50 W: 2.342649, b: -0.866 Cost: 15725.843750\n",
      "Epoch    2/50 W: 2.342649, b: -0.866 Cost: 5059.868652\n",
      "Epoch    3/50 W: 2.342649, b: -0.866 Cost: 1631.707642\n",
      "Epoch    4/50 W: 2.342649, b: -0.866 Cost: 529.858826\n",
      "Epoch    5/50 W: 2.342649, b: -0.866 Cost: 175.710663\n",
      "Epoch    6/50 W: 2.342649, b: -0.866 Cost: 61.881691\n",
      "Epoch    7/50 W: 2.342649, b: -0.866 Cost: 25.293985\n",
      "Epoch    8/50 W: 2.342649, b: -0.866 Cost: 13.532263\n",
      "Epoch    9/50 W: 2.342649, b: -0.866 Cost: 9.749950\n",
      "Epoch   10/50 W: 2.342649, b: -0.866 Cost: 8.532308\n",
      "Epoch   11/50 W: 2.342649, b: -0.866 Cost: 8.138943\n",
      "Epoch   12/50 W: 2.342649, b: -0.866 Cost: 8.010562\n",
      "Epoch   13/50 W: 2.342649, b: -0.866 Cost: 7.967312\n",
      "Epoch   14/50 W: 2.342649, b: -0.866 Cost: 7.951424\n",
      "Epoch   15/50 W: 2.342649, b: -0.866 Cost: 7.944376\n",
      "Epoch   16/50 W: 2.342649, b: -0.866 Cost: 7.940111\n",
      "Epoch   17/50 W: 2.342649, b: -0.866 Cost: 7.936775\n",
      "Epoch   18/50 W: 2.342649, b: -0.866 Cost: 7.933730\n",
      "Epoch   19/50 W: 2.342649, b: -0.866 Cost: 7.930771\n",
      "Epoch   20/50 W: 2.342649, b: -0.866 Cost: 7.927887\n",
      "Epoch   21/50 W: 2.342649, b: -0.866 Cost: 7.924965\n",
      "Epoch   22/50 W: 2.342649, b: -0.866 Cost: 7.922057\n",
      "Epoch   23/50 W: 2.342649, b: -0.866 Cost: 7.919158\n",
      "Epoch   24/50 W: 2.342649, b: -0.866 Cost: 7.916296\n",
      "Epoch   25/50 W: 2.342649, b: -0.866 Cost: 7.913391\n",
      "Epoch   26/50 W: 2.342649, b: -0.866 Cost: 7.910462\n",
      "Epoch   27/50 W: 2.342649, b: -0.866 Cost: 7.907586\n",
      "Epoch   28/50 W: 2.342649, b: -0.866 Cost: 7.904686\n",
      "Epoch   29/50 W: 2.342649, b: -0.866 Cost: 7.901810\n",
      "Epoch   30/50 W: 2.342649, b: -0.866 Cost: 7.898926\n",
      "Epoch   31/50 W: 2.342649, b: -0.866 Cost: 7.895998\n",
      "Epoch   32/50 W: 2.342649, b: -0.866 Cost: 7.893132\n",
      "Epoch   33/50 W: 2.342649, b: -0.866 Cost: 7.890248\n",
      "Epoch   34/50 W: 2.342649, b: -0.866 Cost: 7.887366\n",
      "Epoch   35/50 W: 2.342649, b: -0.866 Cost: 7.884462\n",
      "Epoch   36/50 W: 2.342649, b: -0.866 Cost: 7.881583\n",
      "Epoch   37/50 W: 2.342649, b: -0.866 Cost: 7.878717\n",
      "Epoch   38/50 W: 2.342649, b: -0.866 Cost: 7.875818\n",
      "Epoch   39/50 W: 2.342649, b: -0.866 Cost: 7.872921\n",
      "Epoch   40/50 W: 2.342649, b: -0.866 Cost: 7.870063\n",
      "Epoch   41/50 W: 2.342649, b: -0.866 Cost: 7.867195\n",
      "Epoch   42/50 W: 2.342649, b: -0.866 Cost: 7.864281\n",
      "Epoch   43/50 W: 2.342649, b: -0.866 Cost: 7.861419\n",
      "Epoch   44/50 W: 2.342649, b: -0.866 Cost: 7.858582\n",
      "Epoch   45/50 W: 2.342649, b: -0.866 Cost: 7.855650\n",
      "Epoch   46/50 W: 2.342649, b: -0.866 Cost: 7.852830\n",
      "Epoch   47/50 W: 2.342649, b: -0.866 Cost: 7.849931\n",
      "Epoch   48/50 W: 2.342649, b: -0.866 Cost: 7.847060\n",
      "Epoch   49/50 W: 2.342649, b: -0.866 Cost: 7.844163\n",
      "Epoch   50/50 W: 2.342649, b: -0.866 Cost: 7.841312\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Linear(3,1)\n",
    "#경사하강법 구현 - 내장함수 사용\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "\n",
    "    # H = x_train * w + b\n",
    "    prediction = model(x_train)\n",
    "    cost = criterion(prediction,y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward() #비용함수 미분해서 gradient 계산\n",
    "    optimizer.step() #w,b 업데이트\n",
    "\n",
    "\n",
    "    print('Epoch {:4d}/{} W: {:3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "        epoch, epochs, w.item(), b.item(), cost.item()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16) (101, 1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# torch.manual_seed(777)\n",
    "xy = np.loadtxt('/home/arclab/Documents/동건/rokey/data-04-zoo.csv',delimiter=',',dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "print(x_data.shape,y_data.shape)\n",
    "\n",
    "nb_classes = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data=   [[1. 0. 0. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 1. 0. 1.]\n",
      " [0. 0. 1. ... 1. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 1. 0. 1.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 1. 0. 0.]] \n",
      " X=  tensor([[1., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 0.]]) \n",
      " y_data=  [[0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [3.]\n",
      " [6.]\n",
      " [6.]\n",
      " [6.]\n",
      " [1.]\n",
      " [0.]\n",
      " [3.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [5.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [3.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [3.]\n",
      " [5.]\n",
      " [5.]\n",
      " [1.]\n",
      " [5.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [6.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [5.]\n",
      " [4.]\n",
      " [6.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [6.]\n",
      " [3.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [6.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [6.]\n",
      " [3.]\n",
      " [1.]\n",
      " [0.]\n",
      " [6.]\n",
      " [3.]\n",
      " [1.]\n",
      " [5.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [5.]\n",
      " [0.]\n",
      " [6.]\n",
      " [1.]] \n",
      " Y=  tensor([0, 0, 3, 0, 0, 0, 0, 3, 3, 0, 0, 1, 3, 6, 6, 6, 1, 0, 3, 0, 1, 1, 0, 1,\n",
      "        5, 4, 4, 0, 0, 0, 5, 0, 0, 1, 3, 0, 0, 1, 3, 5, 5, 1, 5, 1, 0, 0, 6, 0,\n",
      "        0, 0, 0, 5, 4, 6, 0, 0, 1, 1, 1, 1, 3, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        6, 3, 0, 0, 2, 6, 1, 1, 2, 6, 3, 1, 0, 6, 3, 1, 5, 4, 2, 2, 3, 0, 0, 1,\n",
      "        0, 5, 0, 6, 1])\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor(x_data) #array를 tensor로 변환\n",
    "Y = torch.tensor(y_data,dtype=torch.long).view(-1)\n",
    "print('x_data=  ',x_data,'\\n','X= ',X,'\\n','y_data= ',y_data,'\\n','Y= ',Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(16,nb_classes)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\tLoss: 2.027\tAcc: 0.00%\n",
      "Step: 100\tLoss: 0.482\tAcc: 87.13%\n",
      "Step: 200\tLoss: 0.328\tAcc: 94.06%\n",
      "Step: 300\tLoss: 0.254\tAcc: 95.05%\n",
      "Step: 400\tLoss: 0.209\tAcc: 95.05%\n",
      "Step: 500\tLoss: 0.178\tAcc: 96.04%\n",
      "Step: 600\tLoss: 0.155\tAcc: 99.01%\n",
      "Step: 700\tLoss: 0.138\tAcc: 99.01%\n",
      "Step: 800\tLoss: 0.124\tAcc: 100.00%\n",
      "Step: 900\tLoss: 0.112\tAcc: 100.00%\n",
      "Step: 1000\tLoss: 0.103\tAcc: 100.00%\n",
      "Step: 1100\tLoss: 0.095\tAcc: 100.00%\n",
      "Step: 1200\tLoss: 0.088\tAcc: 100.00%\n",
      "Step: 1300\tLoss: 0.082\tAcc: 100.00%\n",
      "Step: 1400\tLoss: 0.077\tAcc: 100.00%\n",
      "Step: 1500\tLoss: 0.072\tAcc: 100.00%\n",
      "Step: 1600\tLoss: 0.068\tAcc: 100.00%\n",
      "Step: 1700\tLoss: 0.065\tAcc: 100.00%\n",
      "Step: 1800\tLoss: 0.061\tAcc: 100.00%\n",
      "Step: 1900\tLoss: 0.058\tAcc: 100.00%\n",
      "Step: 2000\tLoss: 0.056\tAcc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "for step in range(2001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "    cost = criterion(hypothesis,Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    prediction = torch.argmax(hypothesis,1)\n",
    "    correct_prediction = (prediction == Y)\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step: {step}\\tLoss: {cost.item():.3f}\\tAcc: {accuracy.item():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n"
     ]
    }
   ],
   "source": [
    "pred = torch.argmax(hypothesis,1)\n",
    "\n",
    "for p,y in zip(pred, Y):\n",
    "    print(f\"[{bool(p.item() == y.item())}] Prediction: {p.item()} True Y: {y.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
